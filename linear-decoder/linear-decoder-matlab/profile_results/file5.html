<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252" /><link rel="stylesheet" href="file:///C:\Program Files\MATLAB\R2012b\toolbox\matlab\codetools\matlab-report-styles.css" type="text/css" /><title>Function details for sparseAutoencoderLinearCost</title><link rel="stylesheet" href="file:///C:\Program Files\MATLAB\R2012b\toolbox\matlab\codetools\matlab-report-styles.css" type="text/css" /></head><body bgcolor="#F8F8F8"><strong>This is a static copy of a profile report</strong><p><a href="file0.html">Home</a><p><span style="font-size:14pt; background:#FFE4B0">sparseAutoencoderLinearCost (615 calls, 1876.199 sec)</span><br/><i>Generated 27-Nov-2013 14:38:40 using cpu time.</i><br/>function in file C:\Users\Raingo\Desktop\advaced-ml\UFLDL\linear-decoder\sparseAutoencoderLinearCost.m<br/>Copy to new window for comparing multiple runs<div class="grayline"/><div class="grayline"/><strong>Parents</strong> (calling functions)<br/><p><table border=0 cellspacing=0 cellpadding=6><tr><td class="td-linebottomrt" bgcolor="#F0F0F0">Function Name</td><td class="td-linebottomrt" bgcolor="#F0F0F0">Function Type</td><td class="td-linebottomrt" bgcolor="#F0F0F0">Calls</td></tr><tr><td class="td-linebottomrt"><a href="file1.html">linearDecoderExercise</a></td><td class="td-linebottomrt">script</td><td class="td-linebottomrt">1</td></tr><tr><td class="td-linebottomrt"><a href="file9.html">...ze,lambda,sparsityParam,beta,patches)</a></td><td class="td-linebottomrt">anonymous function</td><td class="td-linebottomrt">186</td></tr><tr><td class="td-linebottomrt"><a href="file10.html">...ze,lambda,sparsityParam,beta,patches)</a></td><td class="td-linebottomrt">anonymous function</td><td class="td-linebottomrt">428</td></tr></table><div class="grayline"/><strong>Lines where the most time was spent</strong><br/> <p><table border=0 cellspacing=0 cellpadding=6><tr><td class="td-linebottomrt" bgcolor="#F0F0F0">Line Number</td><td class="td-linebottomrt" bgcolor="#F0F0F0">Code</td><td class="td-linebottomrt" bgcolor="#F0F0F0">Calls</td><td class="td-linebottomrt" bgcolor="#F0F0F0">Total Time</td><td class="td-linebottomrt" bgcolor="#F0F0F0">% Time</td><td class="td-linebottomrt" bgcolor="#F0F0F0">Time Plot</td></tr><tr><td class="td-linebottomrt"><a href="#Line52">52</a></td><td class="td-linebottomrt"><pre>[W1grad, W2grad, b1grad, b2gra...</pre></td><td class="td-linebottomrt">615</td><td class="td-linebottomrt">968.280 s</td><td class="td-linebottomrt" class="td-linebottomrt">51.6%</td><td class="td-linebottomrt"><img src="file:///C:\Program Files\MATLAB\R2012b\toolbox\matlab\codetools\private\one-pixel.gif" width=52 height=10></td></tr><tr><td class="td-linebottomrt"><a href="#Line49">49</a></td><td class="td-linebottomrt"><pre>[a2, a3] = twoLayerFF(data, W1...</pre></td><td class="td-linebottomrt">615</td><td class="td-linebottomrt">792.389 s</td><td class="td-linebottomrt" class="td-linebottomrt">42.2%</td><td class="td-linebottomrt"><img src="file:///C:\Program Files\MATLAB\R2012b\toolbox\matlab\codetools\private\one-pixel.gif" width=42 height=10></td></tr><tr><td class="td-linebottomrt"><a href="#Line62">62</a></td><td class="td-linebottomrt"><pre>cost = norm(delta(:), 2) ^ 2;</pre></td><td class="td-linebottomrt">615</td><td class="td-linebottomrt">42.032 s</td><td class="td-linebottomrt" class="td-linebottomrt">2.2%</td><td class="td-linebottomrt"><img src="file:///C:\Program Files\MATLAB\R2012b\toolbox\matlab\codetools\private\one-pixel.gif" width=2 height=10></td></tr><tr><td class="td-linebottomrt"><a href="#Line61">61</a></td><td class="td-linebottomrt"><pre>delta = data - a3;</pre></td><td class="td-linebottomrt">615</td><td class="td-linebottomrt">27.588 s</td><td class="td-linebottomrt" class="td-linebottomrt">1.5%</td><td class="td-linebottomrt"><img src="file:///C:\Program Files\MATLAB\R2012b\toolbox\matlab\codetools\private\one-pixel.gif" width=1 height=10></td></tr><tr><td class="td-linebottomrt"><a href="#Line50">50</a></td><td class="td-linebottomrt"><pre>rho = sum(a2, 2) / nSamples;</pre></td><td class="td-linebottomrt">615</td><td class="td-linebottomrt">22.224 s</td><td class="td-linebottomrt" class="td-linebottomrt">1.2%</td><td class="td-linebottomrt"><img src="file:///C:\Program Files\MATLAB\R2012b\toolbox\matlab\codetools\private\one-pixel.gif" width=1 height=10></td></tr><tr><td class="td-linebottomrt">All other lines</td><td class="td-linebottomrt">&nbsp;</td><td class="td-linebottomrt">&nbsp;</td><td class="td-linebottomrt">23.687 s</td><td class="td-linebottomrt">1.3%</td><td class="td-linebottomrt"><img src="file:///C:\Program Files\MATLAB\R2012b\toolbox\matlab\codetools\private\one-pixel.gif" width=1 height=10></td></tr><tr><td class="td-linebottomrt" bgcolor="#F0F0F0">Totals</td><td class="td-linebottomrt" bgcolor="#F0F0F0">&nbsp;</td><td class="td-linebottomrt" bgcolor="#F0F0F0">&nbsp;</td><td class="td-linebottomrt" bgcolor="#F0F0F0">1876.199 s</td><td class="td-linebottomrt" bgcolor="#F0F0F0">100%</td><td class="td-linebottomrt" bgcolor="#F0F0F0">&nbsp;</td></tr></table><div class="grayline"/><b>Children</b> (called functions)<br/><p><table border=0 cellspacing=0 cellpadding=6><tr><td class="td-linebottomrt" bgcolor="#F0F0F0">Function Name</td><td class="td-linebottomrt" bgcolor="#F0F0F0">Function Type</td><td class="td-linebottomrt" bgcolor="#F0F0F0">Calls</td><td class="td-linebottomrt" bgcolor="#F0F0F0">Total Time</td><td class="td-linebottomrt" bgcolor="#F0F0F0">% Time</td><td class="td-linebottomrt" bgcolor="#F0F0F0">Time Plot</td></tr><tr><td class="td-linebottomrt"><a href="file6.html">sparseAutoencoderLinearCost>twoLayerBP</a></td><td class="td-linebottomrt">subfunction</td><td class="td-linebottomrt">615</td><td class="td-linebottomrt">968.263 s</td><td class="td-linebottomrt">51.6%</td><td class="td-linebottomrt"><img src="file:///C:\Program Files\MATLAB\R2012b\toolbox\matlab\codetools\private\one-pixel.gif" width=52 height=10></td></tr><tr><td class="td-linebottomrt"><a href="file3.html">sparseAutoencoderLinearCost>twoLayerFF</a></td><td class="td-linebottomrt">subfunction</td><td class="td-linebottomrt">615</td><td class="td-linebottomrt">792.374 s</td><td class="td-linebottomrt">42.2%</td><td class="td-linebottomrt"><img src="file:///C:\Program Files\MATLAB\R2012b\toolbox\matlab\codetools\private\one-pixel.gif" width=42 height=10></td></tr><tr><td class="td-linebottomrt"><a href="file8.html">sparseAutoencoderLinearCost>KL</a></td><td class="td-linebottomrt">subfunction</td><td class="td-linebottomrt">615</td><td class="td-linebottomrt">0.039 s</td><td class="td-linebottomrt">0.0%</td><td class="td-linebottomrt"><img src="file:///C:\Program Files\MATLAB\R2012b\toolbox\matlab\codetools\private\one-pixel.gif" width=0 height=10></td></tr><tr><td class="td-linebottomrt">Self time (built-ins, overhead, etc.)</td><td class="td-linebottomrt">&nbsp;</td><td class="td-linebottomrt">&nbsp;</td><td class="td-linebottomrt">115.523 s</td><td class="td-linebottomrt">6.2%</td><td class="td-linebottomrt"><img src="file:///C:\Program Files\MATLAB\R2012b\toolbox\matlab\codetools\private\one-pixel.gif" width=6 height=10></td></tr><tr><td class="td-linebottomrt" bgcolor="#F0F0F0">Totals</td><td class="td-linebottomrt" bgcolor="#F0F0F0">&nbsp;</td><td class="td-linebottomrt" bgcolor="#F0F0F0">&nbsp;</td><td class="td-linebottomrt" bgcolor="#F0F0F0">1876.199 s</td><td class="td-linebottomrt" bgcolor="#F0F0F0">100%</td><td class="td-linebottomrt" bgcolor="#F0F0F0">&nbsp;</td></tr></table><div class="grayline"/><strong>Code Analyzer results</strong><br/><table border=0 cellspacing=0 cellpadding=6><tr><td class="td-linebottomrt" bgcolor="#F0F0F0">Line number</td><td class="td-linebottomrt" bgcolor="#F0F0F0">Message</td></tr></table><div class="grayline"/><strong>Coverage results</strong><br/>Show coverage for parent directory <br/><table border=0 cellspacing=0 cellpadding=6><tr><td class="td-linebottomrt" bgcolor="#F0F0F0">Total lines in function</td><td class="td-linebottomrt">77</td></tr><tr><td class="td-linebottomrt" bgcolor="#F0F0F0">Non-code lines (comments, blank lines)</td><td class="td-linebottomrt">56</td></tr><tr><td class="td-linebottomrt" bgcolor="#F0F0F0">Code lines (lines that can run)</td><td class="td-linebottomrt">21</td></tr><tr><td class="td-linebottomrt" bgcolor="#F0F0F0">Code lines that did run</td><td class="td-linebottomrt">21</td></tr><tr><td class="td-linebottomrt" bgcolor="#F0F0F0">Code lines that did not run</td><td class="td-linebottomrt">0</td></tr><tr><td class="td-linebottomrt" bgcolor="#F0F0F0">Coverage (did run/can run)</td><td class="td-linebottomrt">100.00 %</td></tr></table><div class="grayline"/><b>Function listing</b><br/><pre> <span style="color: #FF0000; font-weight: bold; text-decoration: none">  time</span> <span style="color: #0000FF; font-weight: bold; text-decoration: none">  calls</span>  <span style="font-weight: bold; text-decoration: none">line</span><br/>               <span style="color: #A0A0A0">   1</span> <span style="color: #A0A0A0; background: #FFFFFF;">function [cost,grad] = sparseAutoencoderLinearCost(theta, visibleSize, hiddenSize, ...</span><br/>               <span style="color: #A0A0A0">   2</span> <span style="color: #A0A0A0; background: #FFFFFF;">                                             lambda, sparsityParam, beta, data)</span><br/>               <span style="color: #A0A0A0">   3</span> <span style="color: #228B22; background: #FFFFFF;"></span><br/>               <span style="color: #A0A0A0">   4</span> <span style="color: #228B22; background: #FFFFFF;">% visibleSize: the number of input units (probably 64)</span><br/>               <span style="color: #A0A0A0">   5</span> <span style="color: #228B22; background: #FFFFFF;">% hiddenSize: the number of hidden units (probably 25)</span><br/>               <span style="color: #A0A0A0">   6</span> <span style="color: #228B22; background: #FFFFFF;">% lambda: weight decay parameter</span><br/>               <span style="color: #A0A0A0">   7</span> <span style="color: #228B22; background: #FFFFFF;">% sparsityParam: The desired average activation for the hidden units (denoted in the lecture</span><br/>               <span style="color: #A0A0A0">   8</span> <span style="color: #228B22; background: #FFFFFF;">%                           notes by the greek alphabet rho, which looks like a lower-case "p").</span><br/>               <span style="color: #A0A0A0">   9</span> <span style="color: #228B22; background: #FFFFFF;">% beta: weight of sparsity penalty term</span><br/>               <span style="color: #A0A0A0">  10</span> <span style="color: #228B22; background: #FFFFFF;">% data: Our 64x10000 matrix containing the training data.  So, data(:,i) is the i-th training example.</span><br/>               <span style="color: #A0A0A0">  11</span> <span style="color: #228B22; background: #FFFFFF;"></span><br/>               <span style="color: #A0A0A0">  12</span> <span style="color: #228B22; background: #FFFFFF;">% The input theta is a vector (because minFunc expects the parameters to be a vector).</span><br/>               <span style="color: #A0A0A0">  13</span> <span style="color: #228B22; background: #FFFFFF;">% We first convert theta to the (W1, W2, b1, b2) matrix/vector format, so that this</span><br/>               <span style="color: #A0A0A0">  14</span> <span style="color: #228B22; background: #FFFFFF;">% follows the notation convention of the lecture notes.</span><br/>               <span style="color: #A0A0A0">  15</span> <span style="color: #228B22; background: #FFFFFF;"></span><br/><span style="color: #FF0000">  0.13 </span><span style="color: #0000FF">    615 </span><span style="color: #000000; font-weight: bold">  16</span> <span style="color: #000000; background: #FFFFFF;">W1 = reshape(theta(1:hiddenSize*visibleSize), hiddenSize, visibleSize); </span><br/><span style="color: #FF0000">  0.13 </span><span style="color: #0000FF">    615 </span><span style="color: #000000; font-weight: bold">  17</span> <span style="color: #000000; background: #FFFFFF;">W2 = reshape(theta(hiddenSize*visibleSize+1:2*hiddenSize*visibleSize), visibleSize, hiddenSize); </span><br/><span style="color: #FF0000">&lt; 0.01 </span><span style="color: #0000FF">    615 </span><span style="color: #000000; font-weight: bold">  18</span> <span style="color: #000000; background: #FFFFFF;">b1 = theta(2*hiddenSize*visibleSize+1:2*hiddenSize*visibleSize+hiddenSize); </span><br/><span style="color: #FF0000">&lt; 0.01 </span><span style="color: #0000FF">    615 </span><span style="color: #000000; font-weight: bold">  19</span> <span style="color: #000000; background: #FFFFFF;">b2 = theta(2*hiddenSize*visibleSize+hiddenSize+1:end); </span><br/>               <span style="color: #A0A0A0">  20</span> <span style="color: #228B22; background: #FFFFFF;"></span><br/>               <span style="color: #A0A0A0">  21</span> <span style="color: #228B22; background: #FFFFFF;">% Cost and gradient variables (your code needs to compute these values).</span><br/>               <span style="color: #A0A0A0">  22</span> <span style="color: #228B22; background: #FFFFFF;">% Here, we initialize them to zeros.</span><br/>               <span style="color: #A0A0A0">  23</span> <span style="color: #228B22; background: #FFFFFF;">% cost = 0;</span><br/>               <span style="color: #A0A0A0">  24</span> <span style="color: #228B22; background: #FFFFFF;">% W1grad = zeros(size(W1));</span><br/>               <span style="color: #A0A0A0">  25</span> <span style="color: #228B22; background: #FFFFFF;">% W2grad = zeros(size(W2));</span><br/>               <span style="color: #A0A0A0">  26</span> <span style="color: #228B22; background: #FFFFFF;">% b1grad = zeros(size(b1));</span><br/>               <span style="color: #A0A0A0">  27</span> <span style="color: #228B22; background: #FFFFFF;">% b2grad = zeros(size(b2));</span><br/>               <span style="color: #A0A0A0">  28</span> <span style="color: #228B22; background: #FFFFFF;"></span><br/>               <span style="color: #A0A0A0">  29</span> <span style="color: #228B22; background: #FFFFFF;">%% ---------- YOUR CODE HERE --------------------------------------</span><br/>               <span style="color: #A0A0A0">  30</span> <span style="color: #228B22; background: #FFFFFF;">%  Instructions: Compute the cost/optimization objective J_sparse(W,b) for the Sparse Autoencoder,</span><br/>               <span style="color: #A0A0A0">  31</span> <span style="color: #228B22; background: #FFFFFF;">%                and the corresponding gradients W1grad, W2grad, b1grad, b2grad.</span><br/>               <span style="color: #A0A0A0">  32</span> <span style="color: #228B22; background: #FFFFFF;">%</span><br/>               <span style="color: #A0A0A0">  33</span> <span style="color: #228B22; background: #FFFFFF;">% W1grad, W2grad, b1grad and b2grad should be computed using backpropagation.</span><br/>               <span style="color: #A0A0A0">  34</span> <span style="color: #228B22; background: #FFFFFF;">% Note that W1grad has the same dimensions as W1, b1grad has the same dimensions</span><br/>               <span style="color: #A0A0A0">  35</span> <span style="color: #228B22; background: #FFFFFF;">% as b1, etc.  Your code should set W1grad to be the partial derivative of J_sparse(W,b) with</span><br/>               <span style="color: #A0A0A0">  36</span> <span style="color: #228B22; background: #FFFFFF;">% respect to W1.  I.e., W1grad(i,j) should be the partial derivative of J_sparse(W,b)</span><br/>               <span style="color: #A0A0A0">  37</span> <span style="color: #228B22; background: #FFFFFF;">% with respect to the input parameter W1(i,j).  Thus, W1grad should be equal to the term</span><br/>               <span style="color: #A0A0A0">  38</span> <span style="color: #228B22; background: #FFFFFF;">% [(1/m) \Delta W^{(1)} + \lambda W^{(1)}] in the last block of pseudo-code in Section 2.2</span><br/>               <span style="color: #A0A0A0">  39</span> <span style="color: #228B22; background: #FFFFFF;">% of the lecture notes (and similarly for W2grad, b1grad, b2grad).</span><br/>               <span style="color: #A0A0A0">  40</span> <span style="color: #228B22; background: #FFFFFF;">%</span><br/>               <span style="color: #A0A0A0">  41</span> <span style="color: #228B22; background: #FFFFFF;">% Stated differently, if we were using batch gradient descent to optimize the parameters,</span><br/>               <span style="color: #A0A0A0">  42</span> <span style="color: #228B22; background: #FFFFFF;">% the gradient descent update to W1 would be W1 := W1 - alpha * W1grad, and similarly for W2, b1, b2.</span><br/>               <span style="color: #A0A0A0">  43</span> <span style="color: #228B22; background: #FFFFFF;">%</span><br/>               <span style="color: #A0A0A0">  44</span> <span style="color: #228B22; background: #FFFFFF;"></span><br/><span style="color: #FF0000">  0.01 </span><span style="color: #0000FF">    615 </span><span style="color: #000000; font-weight: bold">  45</span> <span style="color: #000000; background: #FFFFFF;">assert(size(data, 1) == visibleSize); </span><br/>               <span style="color: #A0A0A0">  46</span> <span style="color: #228B22; background: #FFFFFF;"></span><br/>       <span style="color: #0000FF">    615 </span><span style="color: #000000; font-weight: bold">  47</span> <span style="color: #000000; background: #FFFFFF;">nSamples = size(data, 2); </span><br/>               <span style="color: #A0A0A0">  48</span> <span style="color: #228B22; background: #FFFFFF;"></span><br/><span style="color: #FF0000"> 792.39 </span><span style="color: #0000FF">    615 </span><span style="color: #000000; font-weight: bold">  49</span> <a name="Line49"></a><span style="color: #000000; background: #FF8E8E;">[a2, a3] = <a href="file3.html">twoLayerFF</a>(data, W1, W2, b1, b2); </span><br/><span style="color: #FF0000"> 22.22 </span><span style="color: #0000FF">    615 </span><span style="color: #000000; font-weight: bold">  50</span> <a name="Line50"></a><span style="color: #000000; background: #FFFFFF;">rho = sum(a2, 2) / nSamples; </span><br/>               <span style="color: #A0A0A0">  51</span> <span style="color: #228B22; background: #FFFFFF;"></span><br/><span style="color: #FF0000"> 968.28 </span><span style="color: #0000FF">    615 </span><span style="color: #000000; font-weight: bold">  52</span> <a name="Line52"></a><span style="color: #000000; background: #FF8080;">[W1grad, W2grad, b1grad, b2grad] = <a href="file6.html">twoLayerBP</a>(data, data, ... </span><br/>               <span style="color: #A0A0A0">  53</span> <span style="color: #000000; background: #FF8080;">    W1, W2, a2, a3,...</span><br/>               <span style="color: #A0A0A0">  54</span> <span style="color: #000000; background: #FF8080;">    rho, beta, sparsityParam);</span><br/>               <span style="color: #A0A0A0">  55</span> <span style="color: #228B22; background: #FFFFFF;"></span><br/><span style="color: #FF0000">  0.14 </span><span style="color: #0000FF">    615 </span><span style="color: #000000; font-weight: bold">  56</span> <span style="color: #000000; background: #FFFFFF;">W1grad = W1grad / nSamples + lambda * W1; </span><br/><span style="color: #FF0000">  0.12 </span><span style="color: #0000FF">    615 </span><span style="color: #000000; font-weight: bold">  57</span> <span style="color: #000000; background: #FFFFFF;">W2grad = W2grad / nSamples + lambda * W2; </span><br/><span style="color: #FF0000">&lt; 0.01 </span><span style="color: #0000FF">    615 </span><span style="color: #000000; font-weight: bold">  58</span> <span style="color: #000000; background: #FFFFFF;">b1grad = b1grad / nSamples; </span><br/>       <span style="color: #0000FF">    615 </span><span style="color: #000000; font-weight: bold">  59</span> <span style="color: #000000; background: #FFFFFF;">b2grad = b2grad / nSamples; </span><br/>               <span style="color: #A0A0A0">  60</span> <span style="color: #228B22; background: #FFFFFF;"></span><br/><span style="color: #FF0000"> 27.59 </span><span style="color: #0000FF">    615 </span><span style="color: #000000; font-weight: bold">  61</span> <a name="Line61"></a><span style="color: #000000; background: #FFFFFF;">delta = data - a3; </span><br/><span style="color: #FF0000"> 42.03 </span><span style="color: #0000FF">    615 </span><span style="color: #000000; font-weight: bold">  62</span> <a name="Line62"></a><span style="color: #000000; background: #FFFFFF;">cost = norm(delta(:), 2) ^ 2; </span><br/><span style="color: #FF0000">&lt; 0.01 </span><span style="color: #0000FF">    615 </span><span style="color: #000000; font-weight: bold">  63</span> <span style="color: #000000; background: #FFFFFF;">cost = cost / nSamples; </span><br/>               <span style="color: #A0A0A0">  64</span> <span style="color: #228B22; background: #FFFFFF;"></span><br/><span style="color: #FF0000">  0.35 </span><span style="color: #0000FF">    615 </span><span style="color: #000000; font-weight: bold">  65</span> <span style="color: #000000; background: #FFFFFF;">cost = cost + lambda * (norm(W1(:), 2) ^ 2 + norm(W2(:), 2) ^ 2); </span><br/><span style="color: #FF0000">&lt; 0.01 </span><span style="color: #0000FF">    615 </span><span style="color: #000000; font-weight: bold">  66</span> <span style="color: #000000; background: #FFFFFF;">cost = cost / 2; </span><br/>               <span style="color: #A0A0A0">  67</span> <span style="color: #228B22; background: #FFFFFF;"></span><br/><span style="color: #FF0000">  0.05 </span><span style="color: #0000FF">    615 </span><span style="color: #000000; font-weight: bold">  68</span> <span style="color: #000000; background: #FFFFFF;">cost = cost + beta * sum(<a href="file8.html">KL</a>(rho, sparsityParam)); </span><br/>               <span style="color: #A0A0A0">  69</span> <span style="color: #228B22; background: #FFFFFF;"></span><br/>               <span style="color: #A0A0A0">  70</span> <span style="color: #228B22; background: #FFFFFF;">%-------------------------------------------------------------------</span><br/>               <span style="color: #A0A0A0">  71</span> <span style="color: #228B22; background: #FFFFFF;">% After computing the cost and gradient, we will convert the gradients back</span><br/>               <span style="color: #A0A0A0">  72</span> <span style="color: #228B22; background: #FFFFFF;">% to a vector format (suitable for minFunc).  Specifically, we will unroll</span><br/>               <span style="color: #A0A0A0">  73</span> <span style="color: #228B22; background: #FFFFFF;">% your gradient matrices into a vector.</span><br/>               <span style="color: #A0A0A0">  74</span> <span style="color: #228B22; background: #FFFFFF;"></span><br/><span style="color: #FF0000">  0.19 </span><span style="color: #0000FF">    615 </span><span style="color: #000000; font-weight: bold">  75</span> <span style="color: #000000; background: #FFFFFF;">grad = [W1grad(:) ; W2grad(:) ; b1grad(:) ; b2grad(:)]; </span><br/>               <span style="color: #A0A0A0">  76</span> <span style="color: #228B22; background: #FFFFFF;"></span><br/><span style="color: #FF0000">&lt; 0.01 </span><span style="color: #0000FF">    615 </span><span style="color: #000000; font-weight: bold">  77</span> <span style="color: #000000; background: #FFFFFF;">end </span><br/></pre><p><p>Other subfunctions in this file are not included in this listing.</body></html>